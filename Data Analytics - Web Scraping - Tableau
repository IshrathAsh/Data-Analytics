# importing libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup
import requests
import pandas as pd
from bs4 import BeautifulSoup

# Define the URL of the page containing the table
url = "https://collegedunia.com/news/e-60-how-many-students-appear-register-for-gate"

# Headers to mimic a request from a browser
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36"
}

# Make a GET request to fetch the raw HTML content with headers
try:
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Ensure we notice bad responses
except requests.exceptions.RequestException as e:
    print(f"Error fetching the URL: {e}")
    exit()

# Parse the content with BeautifulSoup
soup = BeautifulSoup(response.content, 'html.parser')

# Find all tables in the page
tables = soup.find_all('table')

if not tables:
    print("No tables found on the webpage.")
    exit()

# Iterate over each table found
for index, table in enumerate(tables):
    # Use pandas to read the HTML table
    df = pd.read_html(str(table))[0]  # Convert the table to a DataFrame
    
    # Define the output Excel file path
    output_file = f"extracted_table_{index+1}.xlsx"
    
    # Save the DataFrame to an Excel file
    df.to_excel(output_file, index=False)
    print(f"Table {index+1} saved to {output_file}")

print("All tables have been extracted and saved to Excel files.")
